{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lung Segmentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNXTew9nTCnimV9YAX8f1LO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahil301290/Deep-Learning/blob/master/Lung_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Problem Statement\n",
        "###Lungs Segmentation\n",
        "\n",
        "##### Dataset Details: X-ray images in this data set have been acquired from the tuberculosis control program of the Department of Health and Human Services of Montgomery County, MD, USA. This set contains 138 posterior-anterior x-rays, of which 80 x-rays are normal and 58 x-rays are abnormal with manifestations of tuberculosis. All images are de-identified and available in DICOM format. The set covers a wide range of abnormalities, including effusions and miliary patterns. The data set includes radiology readings available as a text file.\n",
        "\n",
        "#####Dataset Link: https://academictorrents.com/details/ac786f74878a5775c81d490b23842fd4736bfe33"
      ],
      "metadata": {
        "id": "F7f9eM1t_gjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The output should be input image, ground truth, and our generated segmentation"
      ],
      "metadata": {
        "id": "jXCyy5_D_Q8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The code is in four parts: Training, Modelling, Metrics, and Evaluation"
      ],
      "metadata": {
        "id": "iuon5ZBh_eLA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training"
      ],
      "metadata": {
        "id": "9houcQAN-ivu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LoBCzZS-RfU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Recall, Precision\n",
        "from model import build_unet\n",
        "from metrics import dice_loss, dice_coef, iou\n",
        "\n",
        "\"\"\" Global parameters \"\"\"\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "def create_dir(path):\n",
        "    \"\"\" Create a directory. \"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "def load_data(path, split=0.1):\n",
        "    images = sorted(glob(os.path.join(path, \"CXR_png\", \"*.png\")))\n",
        "    masks1 = sorted(glob(os.path.join(path, \"ManualMask\", \"leftMask\", \"*.png\")))\n",
        "    masks2 = sorted(glob(os.path.join(path, \"ManualMask\", \"rightMask\", \"*.png\")))\n",
        "\n",
        "    split_size = int(len(images) * split)\n",
        "\n",
        "    train_x, valid_x = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    train_y1, valid_y1 = train_test_split(masks1, test_size=split_size, random_state=42)\n",
        "    train_y2, valid_y2 = train_test_split(masks2, test_size=split_size, random_state=42)\n",
        "\n",
        "    train_x, test_x = train_test_split(train_x, test_size=split_size, random_state=42)\n",
        "    train_y1, test_y1 = train_test_split(train_y1, test_size=split_size, random_state=42)\n",
        "    train_y2, test_y2 = train_test_split(train_y2, test_size=split_size, random_state=42)\n",
        "\n",
        "    return (train_x, train_y1, train_y2), (valid_x, valid_y1, valid_y2), (test_x, test_y1, test_y2)\n",
        "\n",
        "def read_image(path):\n",
        "    x = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/255.0\n",
        "    x = x.astype(np.float32)\n",
        "    return x\n",
        "\n",
        "def read_mask(path1, path2):\n",
        "    x1 = cv2.imread(path1, cv2.IMREAD_GRAYSCALE)\n",
        "    x2 = cv2.imread(path2, cv2.IMREAD_GRAYSCALE)\n",
        "    x = x1 + x2\n",
        "    x = cv2.resize(x, (W, H))\n",
        "    x = x/np.max(x)\n",
        "    x = x > 0.5\n",
        "    x = x.astype(np.float32)\n",
        "    x = np.expand_dims(x, axis=-1)\n",
        "    return x\n",
        "\n",
        "def tf_parse(x, y1, y2):\n",
        "    def _parse(x, y1, y2):\n",
        "        x = x.decode()\n",
        "        y1 = y1.decode()\n",
        "        y2 = y2.decode()\n",
        "\n",
        "        x = read_image(x)\n",
        "        y = read_mask(y1, y2)\n",
        "        return x, y\n",
        "\n",
        "    x, y = tf.numpy_function(_parse, [x, y1, y2], [tf.float32, tf.float32])\n",
        "    x.set_shape([H, W, 3])\n",
        "    y.set_shape([H, W, 1])\n",
        "    return x, y\n",
        "\n",
        "def tf_dataset(X, Y1, Y2, batch=8):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, Y1, Y2))\n",
        "    dataset = dataset.shuffle(buffer_size=200)\n",
        "    dataset = dataset.map(tf_parse)\n",
        "    dataset = dataset.batch(batch)\n",
        "    dataset = dataset.prefetch(4)\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"files\")\n",
        "\n",
        "    \"\"\" Hyperparameters \"\"\"\n",
        "    batch_size = 2\n",
        "    lr = 1e-5\n",
        "    num_epochs = 10\n",
        "    model_path = os.path.join(\"files\", \"model.h5\")\n",
        "    csv_path = os.path.join(\"files\", \"data.csv\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"C:/Users/isaah/Downloads/NLM-MontgomeryCXRSet/MontgomerySet\" \n",
        "    (train_x, train_y1, train_y2), (valid_x, valid_y1, valid_y2), (test_x, test_y1, test_y2) = load_data(dataset_path)\n",
        "\n",
        "    print(f\"Train: {len(train_x)} - {len(train_y1)} - {len(train_y2)}\")\n",
        "    print(f\"Valid: {len(valid_x)} - {len(valid_y1)} - {len(valid_y2)}\")\n",
        "    print(f\"Test: {len(test_x)} - {len(test_y1)} - {len(test_y2)}\")\n",
        "\n",
        "    train_dataset = tf_dataset(train_x, train_y1, train_y2, batch=batch_size)\n",
        "    valid_dataset = tf_dataset(valid_x, valid_y1, valid_y2, batch=batch_size)\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = build_unet((H, W, 3))\n",
        "    metrics = [dice_coef, iou, Recall(), Precision()]\n",
        "    model.compile(loss=dice_loss, optimizer=Adam(lr), metrics=metrics)\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
        "        CSVLogger(csv_path)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=valid_dataset,\n",
        "        callbacks=callbacks\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modelling"
      ],
      "metadata": {
        "id": "Ok21ZA3z-szY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def conv_block(input, num_filters):\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def encoder_block(input, num_filters):\n",
        "    x = conv_block(input, num_filters)\n",
        "    p = MaxPool2D((2, 2))(x)\n",
        "    return x, p\n",
        "\n",
        "def decoder_block(input, skip_features, num_filters):\n",
        "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input)\n",
        "    x = Concatenate()([x, skip_features])\n",
        "    x = conv_block(x, num_filters)\n",
        "    return x\n",
        "\n",
        "def build_unet(input_shape):\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    s1, p1 = encoder_block(inputs, 64)\n",
        "    s2, p2 = encoder_block(p1, 128)\n",
        "    s3, p3 = encoder_block(p2, 256)\n",
        "    s4, p4 = encoder_block(p3, 512)\n",
        "\n",
        "    b1 = conv_block(p4, 1024)\n",
        "\n",
        "    d1 = decoder_block(b1, s4, 512)\n",
        "    d2 = decoder_block(d1, s3, 256)\n",
        "    d3 = decoder_block(d2, s2, 128)\n",
        "    d4 = decoder_block(d3, s1, 64)\n",
        "\n",
        "    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"U-Net\")\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_shape = (512, 512, 3)\n",
        "    model = build_unet(input_shape)\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "nJf9AcMa-tjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Metrics"
      ],
      "metadata": {
        "id": "wMbtoffO-uiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def iou(y_true, y_pred):\n",
        "    def f(y_true, y_pred):\n",
        "        intersection = (y_true * y_pred).sum()\n",
        "        union = y_true.sum() + y_pred.sum() - intersection\n",
        "        x = (intersection + 1e-15) / (union + 1e-15)\n",
        "        x = x.astype(np.float32)\n",
        "        return x\n",
        "    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n",
        "\n",
        "smooth = 1e-15\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = tf.keras.layers.Flatten()(y_true)\n",
        "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
        "    intersection = tf.reduce_sum(y_true * y_pred)\n",
        "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)"
      ],
      "metadata": {
        "id": "Xgi3gV9a-zso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "5wi3VNUw-1h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import CustomObjectScope\n",
        "from metrics import dice_loss, dice_coef, iou\n",
        "from train import load_data, create_dir, tf_dataset\n",
        "\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"results\")\n",
        "\n",
        "    \"\"\" Loading model \"\"\"\n",
        "    with CustomObjectScope({'iou': iou, 'dice_coef': dice_coef, 'dice_loss': dice_loss}):\n",
        "        model = tf.keras.models.load_model(\"files/model.h5\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    dataset_path = \"D:/projects/pneumonia detection/NLM-MontgomeryCXRSet/MontgomerySet\"\n",
        "    (train_x, train_y1, train_y2), (valid_x, valid_y1, valid_y2), (test_x, test_y1, test_y2) = load_data(dataset_path)\n",
        "\n",
        "    \"\"\" Predicting the mask \"\"\"\n",
        "    for x, y1, y2 in tqdm(zip(test_x, test_y1, test_y2), total=len(test_x)):\n",
        "        \"\"\" Extracing the image name. \"\"\"\n",
        "        image_name = x.split(\"/\")[-1]\n",
        "\n",
        "        \"\"\" Reading the image \"\"\"\n",
        "        ori_x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
        "        ori_x = cv2.resize(ori_x, (W, H))\n",
        "        x = ori_x/255.0\n",
        "        x = x.astype(np.float32)\n",
        "        x = np.expand_dims(x, axis=0)\n",
        "\n",
        "        \"\"\" Reading the mask \"\"\"\n",
        "        ori_y1 = cv2.imread(y1, cv2.IMREAD_GRAYSCALE)\n",
        "        ori_y2 = cv2.imread(y2, cv2.IMREAD_GRAYSCALE)\n",
        "        ori_y = ori_y1 + ori_y2\n",
        "        ori_y = cv2.resize(ori_y, (W, H))\n",
        "        ori_y = np.expand_dims(ori_y, axis=-1)  ## (512, 512, 1)\n",
        "        ori_y = np.concatenate([ori_y, ori_y, ori_y], axis=-1)  ## (512, 512, 3)\n",
        "\n",
        "        \"\"\" Predicting the mask. \"\"\"\n",
        "        y_pred = model.predict(x)[0] > 0.5\n",
        "        y_pred = y_pred.astype(np.int32)\n",
        "\n",
        "        \"\"\" Saving the predicted mask along with the image and GT \"\"\"\n",
        "        save_image_path = f\"results/{image_name}\"\n",
        "        y_pred = np.concatenate([y_pred, y_pred, y_pred], axis=-1)\n",
        "\n",
        "        sep_line = np.ones((H, 10, 3)) * 255\n",
        "\n",
        "        cat_image = np.concatenate([ori_x, sep_line, ori_y, sep_line, y_pred*255], axis=1)\n",
        "        cv2.imwrite(save_image_path, cat_image)"
      ],
      "metadata": {
        "id": "ghxdy0LT_C1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}